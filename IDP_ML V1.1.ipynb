{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o96DMYltjZ9W"
      },
      "source": [
        "# Import Packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: numpy in c:\\users\\selam\\idp\\.venv\\lib\\site-packages (1.25.2)\n",
            "Requirement already satisfied: pandas in c:\\users\\selam\\idp\\.venv\\lib\\site-packages (2.0.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\selam\\idp\\.venv\\lib\\site-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in c:\\users\\selam\\idp\\.venv\\lib\\site-packages (from pandas) (2023.3)\n",
            "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\selam\\idp\\.venv\\lib\\site-packages (from pandas) (2023.3)\n",
            "Requirement already satisfied: numpy>=1.21.0 in c:\\users\\selam\\idp\\.venv\\lib\\site-packages (from pandas) (1.25.2)\n",
            "Requirement already satisfied: six>=1.5 in c:\\users\\selam\\idp\\.venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
            "Requirement already satisfied: geopandas in c:\\users\\selam\\idp\\.venv\\lib\\site-packages (0.13.2)\n",
            "Requirement already satisfied: fiona>=1.8.19 in c:\\users\\selam\\idp\\.venv\\lib\\site-packages (from geopandas) (1.9.4.post1)\n",
            "Requirement already satisfied: packaging in c:\\users\\selam\\idp\\.venv\\lib\\site-packages (from geopandas) (23.1)\n",
            "Requirement already satisfied: pandas>=1.1.0 in c:\\users\\selam\\idp\\.venv\\lib\\site-packages (from geopandas) (2.0.3)\n",
            "Requirement already satisfied: pyproj>=3.0.1 in c:\\users\\selam\\idp\\.venv\\lib\\site-packages (from geopandas) (3.6.0)\n",
            "Requirement already satisfied: shapely>=1.7.1 in c:\\users\\selam\\idp\\.venv\\lib\\site-packages (from geopandas) (2.0.1)\n",
            "Requirement already satisfied: attrs>=19.2.0 in c:\\users\\selam\\idp\\.venv\\lib\\site-packages (from fiona>=1.8.19->geopandas) (23.1.0)\n",
            "Requirement already satisfied: certifi in c:\\users\\selam\\idp\\.venv\\lib\\site-packages (from fiona>=1.8.19->geopandas) (2023.7.22)\n",
            "Requirement already satisfied: click~=8.0 in c:\\users\\selam\\idp\\.venv\\lib\\site-packages (from fiona>=1.8.19->geopandas) (8.1.7)\n",
            "Requirement already satisfied: click-plugins>=1.0 in c:\\users\\selam\\idp\\.venv\\lib\\site-packages (from fiona>=1.8.19->geopandas) (1.1.1)\n",
            "Requirement already satisfied: cligj>=0.5 in c:\\users\\selam\\idp\\.venv\\lib\\site-packages (from fiona>=1.8.19->geopandas) (0.7.2)\n",
            "Requirement already satisfied: six in c:\\users\\selam\\idp\\.venv\\lib\\site-packages (from fiona>=1.8.19->geopandas) (1.16.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\selam\\idp\\.venv\\lib\\site-packages (from pandas>=1.1.0->geopandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in c:\\users\\selam\\idp\\.venv\\lib\\site-packages (from pandas>=1.1.0->geopandas) (2023.3)\n",
            "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\selam\\idp\\.venv\\lib\\site-packages (from pandas>=1.1.0->geopandas) (2023.3)\n",
            "Requirement already satisfied: numpy>=1.21.0 in c:\\users\\selam\\idp\\.venv\\lib\\site-packages (from pandas>=1.1.0->geopandas) (1.25.2)\n",
            "Requirement already satisfied: colorama in c:\\users\\selam\\idp\\.venv\\lib\\site-packages (from click~=8.0->fiona>=1.8.19->geopandas) (0.4.6)\n",
            "Requirement already satisfied: regex in c:\\users\\selam\\idp\\.venv\\lib\\site-packages (2023.8.8)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ERROR: Could not find a version that satisfies the requirement nltk.corpus (from versions: none)\n",
            "ERROR: No matching distribution found for nltk.corpus\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: unidecode in c:\\users\\selam\\idp\\.venv\\lib\\site-packages (1.3.6)\n",
            "Requirement already satisfied: nltk in c:\\users\\selam\\idp\\.venv\\lib\\site-packages (3.8.1)\n",
            "Requirement already satisfied: click in c:\\users\\selam\\idp\\.venv\\lib\\site-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in c:\\users\\selam\\idp\\.venv\\lib\\site-packages (from nltk) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\selam\\idp\\.venv\\lib\\site-packages (from nltk) (2023.8.8)\n",
            "Requirement already satisfied: tqdm in c:\\users\\selam\\idp\\.venv\\lib\\site-packages (from nltk) (4.66.1)\n",
            "Requirement already satisfied: colorama in c:\\users\\selam\\idp\\.venv\\lib\\site-packages (from click->nltk) (0.4.6)\n",
            "Requirement already satisfied: scikit-learn in c:\\users\\selam\\idp\\.venv\\lib\\site-packages (1.3.0)\n",
            "Requirement already satisfied: numpy>=1.17.3 in c:\\users\\selam\\idp\\.venv\\lib\\site-packages (from scikit-learn) (1.25.2)\n",
            "Requirement already satisfied: scipy>=1.5.0 in c:\\users\\selam\\idp\\.venv\\lib\\site-packages (from scikit-learn) (1.11.2)\n",
            "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\selam\\idp\\.venv\\lib\\site-packages (from scikit-learn) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\selam\\idp\\.venv\\lib\\site-packages (from scikit-learn) (3.2.0)\n",
            "Requirement already satisfied: matplotlib in c:\\users\\selam\\idp\\.venv\\lib\\site-packages (3.7.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\selam\\idp\\.venv\\lib\\site-packages (from matplotlib) (1.1.0)\n",
            "Requirement already satisfied: cycler>=0.10 in c:\\users\\selam\\idp\\.venv\\lib\\site-packages (from matplotlib) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\selam\\idp\\.venv\\lib\\site-packages (from matplotlib) (4.42.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\selam\\idp\\.venv\\lib\\site-packages (from matplotlib) (1.4.5)\n",
            "Requirement already satisfied: numpy>=1.20 in c:\\users\\selam\\idp\\.venv\\lib\\site-packages (from matplotlib) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in c:\\users\\selam\\idp\\.venv\\lib\\site-packages (from matplotlib) (23.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\selam\\idp\\.venv\\lib\\site-packages (from matplotlib) (10.0.0)\n",
            "Requirement already satisfied: pyparsing<3.1,>=2.3.1 in c:\\users\\selam\\idp\\.venv\\lib\\site-packages (from matplotlib) (3.0.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\selam\\idp\\.venv\\lib\\site-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in c:\\users\\selam\\idp\\.venv\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
            "Requirement already satisfied: seaborn in c:\\users\\selam\\idp\\.venv\\lib\\site-packages (0.12.2)\n",
            "Requirement already satisfied: numpy!=1.24.0,>=1.17 in c:\\users\\selam\\idp\\.venv\\lib\\site-packages (from seaborn) (1.25.2)\n",
            "Requirement already satisfied: pandas>=0.25 in c:\\users\\selam\\idp\\.venv\\lib\\site-packages (from seaborn) (2.0.3)\n",
            "Requirement already satisfied: matplotlib!=3.6.1,>=3.1 in c:\\users\\selam\\idp\\.venv\\lib\\site-packages (from seaborn) (3.7.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\selam\\idp\\.venv\\lib\\site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (1.1.0)\n",
            "Requirement already satisfied: cycler>=0.10 in c:\\users\\selam\\idp\\.venv\\lib\\site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\selam\\idp\\.venv\\lib\\site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (4.42.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\selam\\idp\\.venv\\lib\\site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in c:\\users\\selam\\idp\\.venv\\lib\\site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (23.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\selam\\idp\\.venv\\lib\\site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (10.0.0)\n",
            "Requirement already satisfied: pyparsing<3.1,>=2.3.1 in c:\\users\\selam\\idp\\.venv\\lib\\site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (3.0.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\selam\\idp\\.venv\\lib\\site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in c:\\users\\selam\\idp\\.venv\\lib\\site-packages (from pandas>=0.25->seaborn) (2023.3)\n",
            "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\selam\\idp\\.venv\\lib\\site-packages (from pandas>=0.25->seaborn) (2023.3)\n",
            "Requirement already satisfied: six>=1.5 in c:\\users\\selam\\idp\\.venv\\lib\\site-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.1->seaborn) (1.16.0)\n",
            "Requirement already satisfied: wordcloud in c:\\users\\selam\\idp\\.venv\\lib\\site-packages (1.9.2)\n",
            "Requirement already satisfied: numpy>=1.6.1 in c:\\users\\selam\\idp\\.venv\\lib\\site-packages (from wordcloud) (1.25.2)\n",
            "Requirement already satisfied: pillow in c:\\users\\selam\\idp\\.venv\\lib\\site-packages (from wordcloud) (10.0.0)\n",
            "Requirement already satisfied: matplotlib in c:\\users\\selam\\idp\\.venv\\lib\\site-packages (from wordcloud) (3.7.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\selam\\idp\\.venv\\lib\\site-packages (from matplotlib->wordcloud) (1.1.0)\n",
            "Requirement already satisfied: cycler>=0.10 in c:\\users\\selam\\idp\\.venv\\lib\\site-packages (from matplotlib->wordcloud) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\selam\\idp\\.venv\\lib\\site-packages (from matplotlib->wordcloud) (4.42.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\selam\\idp\\.venv\\lib\\site-packages (from matplotlib->wordcloud) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in c:\\users\\selam\\idp\\.venv\\lib\\site-packages (from matplotlib->wordcloud) (23.1)\n",
            "Requirement already satisfied: pyparsing<3.1,>=2.3.1 in c:\\users\\selam\\idp\\.venv\\lib\\site-packages (from matplotlib->wordcloud) (3.0.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\selam\\idp\\.venv\\lib\\site-packages (from matplotlib->wordcloud) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in c:\\users\\selam\\idp\\.venv\\lib\\site-packages (from python-dateutil>=2.7->matplotlib->wordcloud) (1.16.0)\n",
            "Requirement already satisfied: folium in c:\\users\\selam\\idp\\.venv\\lib\\site-packages (0.14.0)\n",
            "Requirement already satisfied: branca>=0.6.0 in c:\\users\\selam\\idp\\.venv\\lib\\site-packages (from folium) (0.6.0)\n",
            "Requirement already satisfied: jinja2>=2.9 in c:\\users\\selam\\idp\\.venv\\lib\\site-packages (from folium) (3.1.2)\n",
            "Requirement already satisfied: numpy in c:\\users\\selam\\idp\\.venv\\lib\\site-packages (from folium) (1.25.2)\n",
            "Requirement already satisfied: requests in c:\\users\\selam\\idp\\.venv\\lib\\site-packages (from folium) (2.31.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\selam\\idp\\.venv\\lib\\site-packages (from jinja2>=2.9->folium) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\selam\\idp\\.venv\\lib\\site-packages (from requests->folium) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\selam\\idp\\.venv\\lib\\site-packages (from requests->folium) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\selam\\idp\\.venv\\lib\\site-packages (from requests->folium) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\selam\\idp\\.venv\\lib\\site-packages (from requests->folium) (2023.7.22)\n",
            "Requirement already satisfied: branca in c:\\users\\selam\\idp\\.venv\\lib\\site-packages (0.6.0)\n",
            "Requirement already satisfied: jinja2 in c:\\users\\selam\\idp\\.venv\\lib\\site-packages (from branca) (3.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\selam\\idp\\.venv\\lib\\site-packages (from jinja2->branca) (2.1.3)\n"
          ]
        }
      ],
      "source": [
        "!pip3 install numpy\n",
        "!pip3 install pandas\n",
        "!pip3 install geopandas\n",
        "!pip3 install regex\n",
        "!pip3 install unidecode\n",
        "!pip3 install nltk\n",
        "!pip3 install scikit-learn\n",
        "!pip3 install matplotlib\n",
        "!pip3 install seaborn\n",
        "!pip3 install wordcloud\n",
        "!pip3 install folium\n",
        "!pip3 install branca\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "17M9BgUTjiln",
        "outputId": "7716d009-08f8-4992-f466-5786024f074d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\selam\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     C:\\Users\\selam\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "# Data Structures\n",
        "import numpy  as np\n",
        "import pandas as pd\n",
        "import geopandas as gpd\n",
        "import json\n",
        "\n",
        "# Corpus Processing\n",
        "import re\n",
        "import nltk.corpus\n",
        "from unidecode import unidecode\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk import SnowballStemmer\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.preprocessing import normalize\n",
        "\n",
        "# K-Means\n",
        "from sklearn import cluster\n",
        "\n",
        "# Visualization and Analysis\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.cm as cm\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import silhouette_samples, silhouette_score\n",
        "from wordcloud import WordCloud\n",
        "\n",
        "# Map Viz\n",
        "import folium\n",
        "#import branca.colormap as cm\n",
        "from branca.element import Figure"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kSBjlKSOiCnQ"
      },
      "source": [
        "# Convert CSV to information fo ML Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "A4yk1VAtiXYX",
        "outputId": "fb19a3eb-96ca-48bb-e5a4-fb0fcb99a150"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ocr_doc_page_num</th>\n",
              "      <th>ll_doc_name_reformatted</th>\n",
              "      <th>ll_doc_start_page</th>\n",
              "      <th>ll_doc_stop_page</th>\n",
              "      <th>ocr_text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>Hazard Insurance</td>\n",
              "      <td>539</td>\n",
              "      <td>542</td>\n",
              "      <td>['07/19/21 16:36:59 800-776-4737', 'Veterans U...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>Hazard Insurance</td>\n",
              "      <td>539</td>\n",
              "      <td>542</td>\n",
              "      <td>['07/19/21 16:37:33 800-776-4737', 'Veterans U...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>Hazard Insurance</td>\n",
              "      <td>539</td>\n",
              "      <td>542</td>\n",
              "      <td>['07/19/21 16:38:06 800-776-4737', 'Veterans U...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>Hazard Insurance</td>\n",
              "      <td>212</td>\n",
              "      <td>222</td>\n",
              "      <td>['Renewal House &amp; Home Policy Declarations', '...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2</td>\n",
              "      <td>Hazard Insurance</td>\n",
              "      <td>212</td>\n",
              "      <td>222</td>\n",
              "      <td>['Renewal House &amp; Home Policy Declarations', '...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   ocr_doc_page_num ll_doc_name_reformatted  ll_doc_start_page  \\\n",
              "0                 1        Hazard Insurance                539   \n",
              "1                 2        Hazard Insurance                539   \n",
              "2                 3        Hazard Insurance                539   \n",
              "3                 1        Hazard Insurance                212   \n",
              "4                 2        Hazard Insurance                212   \n",
              "\n",
              "   ll_doc_stop_page                                           ocr_text  \n",
              "0               542  ['07/19/21 16:36:59 800-776-4737', 'Veterans U...  \n",
              "1               542  ['07/19/21 16:37:33 800-776-4737', 'Veterans U...  \n",
              "2               542  ['07/19/21 16:38:06 800-776-4737', 'Veterans U...  \n",
              "3               222  ['Renewal House & Home Policy Declarations', '...  \n",
              "4               222  ['Renewal House & Home Policy Declarations', '...  "
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv(\"HazardInsurance.csv\")\n",
        "dataset = df[[\"ocr_doc_page_num\", \"ll_doc_name_reformatted\", \"ll_doc_start_page\", \"ll_doc_stop_page\",\"ocr_text\" ]]\n",
        "dataset.head()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Corpus Loading:\n",
        "we'll extract the ocr text column into a list of texts for our corpus."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "eXUSTEvRl77E"
      },
      "outputs": [],
      "source": [
        "corpus = dataset['ocr_text'].tolist()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Corpus Processing\n",
        "## 1. Stop Words and Stemming\n",
        "We will do a data engineering routine with our anthems dataset so later we can make a good statistical model. In order to do so, we'll remove all words that don't contribute to the semantic meaning of the text (words that are not within the english alphabet) and keep all of the remaining words in the simplest format possible, so we can apply a function that gives weights to each word without generating any bias or outliers. To do that there are many techniques to clean up our corpus, among them we will remove the most common words (stop words) and apply stemming, a technique that reduces a word to it's root.\n",
        "\n",
        "The methods that apply stemming and stop words removal are listed bellow. We will also define a method that removes any words with less than 2 letters or more than 21 letters to clean our corpus even more."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "c6T7bjXImC4D"
      },
      "outputs": [],
      "source": [
        "def removeWords(listOfTokens, listOfWords):\n",
        "    return [token for token in listOfTokens if token not in listOfWords]\n",
        "\n",
        "def applyStemming(listOfTokens, stemmer):\n",
        "    return [stemmer.stem(token) for token in listOfTokens]\n",
        "\n",
        "def twoLetters(listOfTokens):\n",
        "    twoLetterWord = []\n",
        "    for token in listOfTokens:\n",
        "        if len(token) <= 2 or len(token) >= 21:\n",
        "            twoLetterWord.append(token)\n",
        "    return twoLetterWord"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. The main corpus processing function.\n",
        "A section back, at the exploration of our dataset, we noticed some words containg weird characters that should be removed. By using RegEx our main processing function will remove unknown ASCII symbols, especial chars, numbers, e-mails, URLs, etc (It's a bit of a overkill, I know). It also uses the auxiliary funcitions defined above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "bDlyPPl7mT9D"
      },
      "outputs": [],
      "source": [
        "def processCorpus(corpus, language):\n",
        "    stopwords = nltk.corpus.stopwords.words(language)\n",
        "    param_stemmer = SnowballStemmer(language)\n",
        "    common_words = ['Policy', 'policy']\n",
        "\n",
        "    for document in corpus:\n",
        "        index = corpus.index(document)\n",
        "        corpus[index] = corpus[index].replace(u'\\ufffd', '8')   # Replaces the ASCII '�' symbol with '8'\n",
        "        corpus[index] = corpus[index].replace(',', '')          # Removes commas\n",
        "        corpus[index] = corpus[index].rstrip('\\n')              # Removes line breaks\n",
        "        corpus[index] = corpus[index].casefold()                # Makes all letters lowercase\n",
        "\n",
        "        corpus[index] = re.sub('\\W_',' ', corpus[index])        # removes specials characters and leaves only words\n",
        "        corpus[index] = re.sub(\"\\S*\\d\\S*\",\" \", corpus[index])   # removes numbers and words concatenated with numbers IE h4ck3r. Removes road names such as BR-381.\n",
        "        corpus[index] = re.sub(\"\\S*@\\S*\\s?\",\" \", corpus[index]) # removes emails and mentions (words with @)\n",
        "        corpus[index] = re.sub(r'http\\S+', '', corpus[index])   # removes URLs with http\n",
        "        corpus[index] = re.sub(r'www\\S+', '', corpus[index])    # removes URLs with www\n",
        "\n",
        "        listOfTokens = word_tokenize(corpus[index])\n",
        "        twoLetterWord = twoLetters(listOfTokens)\n",
        "\n",
        "        listOfTokens = removeWords(listOfTokens, stopwords)\n",
        "        listOfTokens = removeWords(listOfTokens, twoLetterWord)\n",
        "        listOfTokens = applyStemming(listOfTokens, param_stemmer)\n",
        "        listOfTokens = removeWords(listOfTokens, common_words)\n",
        "\n",
        "        corpus[index]   = \" \".join(listOfTokens)\n",
        "        corpus[index] = unidecode(corpus[index])\n",
        "\n",
        "    return corpus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "Q8PO9TYinX5C"
      },
      "outputs": [],
      "source": [
        "language = 'english'\n",
        "corpus = processCorpus(corpus, language)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "23XjduXpofYO",
        "outputId": "4da3b526-8bc1-4e50-ab9e-7860ebf4cd61"
      },
      "outputs": [
        {
          "ename": "MemoryError",
          "evalue": "Unable to allocate 11.6 GiB for an array with shape (37797, 41261) and data type float64",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[15], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m vectorizer \u001b[39m=\u001b[39m TfidfVectorizer()\n\u001b[0;32m      2\u001b[0m X \u001b[39m=\u001b[39m vectorizer\u001b[39m.\u001b[39mfit_transform(corpus)\n\u001b[1;32m----> 3\u001b[0m tf_idf \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame(data \u001b[39m=\u001b[39m X\u001b[39m.\u001b[39;49mtoarray(), columns\u001b[39m=\u001b[39mvectorizer\u001b[39m.\u001b[39mget_feature_names_out())\n\u001b[0;32m      5\u001b[0m final_df \u001b[39m=\u001b[39m tf_idf\n\u001b[0;32m      7\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m rows\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(final_df\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]))\n",
            "File \u001b[1;32mc:\\Users\\selam\\IDP\\.venv\\lib\\site-packages\\scipy\\sparse\\_compressed.py:1050\u001b[0m, in \u001b[0;36m_cs_matrix.toarray\u001b[1;34m(self, order, out)\u001b[0m\n\u001b[0;32m   1048\u001b[0m \u001b[39mif\u001b[39;00m out \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m order \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   1049\u001b[0m     order \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_swap(\u001b[39m'\u001b[39m\u001b[39mcf\u001b[39m\u001b[39m'\u001b[39m)[\u001b[39m0\u001b[39m]\n\u001b[1;32m-> 1050\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_process_toarray_args(order, out)\n\u001b[0;32m   1051\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (out\u001b[39m.\u001b[39mflags\u001b[39m.\u001b[39mc_contiguous \u001b[39mor\u001b[39;00m out\u001b[39m.\u001b[39mflags\u001b[39m.\u001b[39mf_contiguous):\n\u001b[0;32m   1052\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mOutput array must be C or F contiguous\u001b[39m\u001b[39m'\u001b[39m)\n",
            "File \u001b[1;32mc:\\Users\\selam\\IDP\\.venv\\lib\\site-packages\\scipy\\sparse\\_base.py:1267\u001b[0m, in \u001b[0;36m_spbase._process_toarray_args\u001b[1;34m(self, order, out)\u001b[0m\n\u001b[0;32m   1265\u001b[0m     \u001b[39mreturn\u001b[39;00m out\n\u001b[0;32m   1266\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1267\u001b[0m     \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39;49mzeros(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mshape, dtype\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdtype, order\u001b[39m=\u001b[39;49morder)\n",
            "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 11.6 GiB for an array with shape (37797, 41261) and data type float64"
          ]
        }
      ],
      "source": [
        "vectorizer = TfidfVectorizer()\n",
        "X = vectorizer.fit_transform(corpus)\n",
        "tf_idf = pd.DataFrame(data = X.toarray(), columns=vectorizer.get_feature_names_out())\n",
        "\n",
        "final_df = tf_idf\n",
        "\n",
        "print(\"{} rows\".format(final_df.shape[0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# first 5 words with highest weight on document 0:\n",
        "final_df.T.nlargest(5, 0)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# K-Means\n",
        "Function that runs the K-Means algorithm max_k times and returns a dictionary of each k result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def run_KMeans(max_k, data):\n",
        "    max_k += 1\n",
        "    kmeans_results = dict()\n",
        "    for k in range(2 , max_k):\n",
        "        kmeans = cluster.KMeans(n_clusters = k\n",
        "                               , init = 'k-means++'\n",
        "                               , n_init = 10\n",
        "                               , tol = 0.0001\n",
        "                               , n_jobs = -1\n",
        "                               , random_state = 1\n",
        "                               , algorithm = 'full')\n",
        "\n",
        "        kmeans_results.update( {k : kmeans.fit(data)} )\n",
        "        \n",
        "    return kmeans_results"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Silhouette Score\n",
        "The silhouette value is a measure of how similar an object is to its own cluster (cohesion) compared to other clusters (separation)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def printAvg(avg_dict):\n",
        "    for avg in sorted(avg_dict.keys(), reverse=True):\n",
        "        print(\"Avg: {}\\tK:{}\".format(avg.round(4), avg_dict[avg]))\n",
        "        \n",
        "def plotSilhouette(df, n_clusters, kmeans_labels, silhouette_avg):\n",
        "    fig, ax1 = plt.subplots(1)\n",
        "    fig.set_size_inches(8, 6)\n",
        "    ax1.set_xlim([-0.2, 1])\n",
        "    ax1.set_ylim([0, len(df) + (n_clusters + 1) * 10])\n",
        "    \n",
        "    ax1.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\") # The vertical line for average silhouette score of all the values\n",
        "    ax1.set_yticks([])  # Clear the yaxis labels / ticks\n",
        "    ax1.set_xticks([-0.2, 0, 0.2, 0.4, 0.6, 0.8, 1])\n",
        "    plt.title((\"Silhouette analysis for K = %d\" % n_clusters), fontsize=10, fontweight='bold')\n",
        "    \n",
        "    y_lower = 10\n",
        "    sample_silhouette_values = silhouette_samples(df, kmeans_labels) # Compute the silhouette scores for each sample\n",
        "    for i in range(n_clusters):\n",
        "        ith_cluster_silhouette_values = sample_silhouette_values[kmeans_labels == i]\n",
        "        ith_cluster_silhouette_values.sort()\n",
        "\n",
        "        size_cluster_i = ith_cluster_silhouette_values.shape[0]\n",
        "        y_upper = y_lower + size_cluster_i\n",
        "\n",
        "        color = cm.nipy_spectral(float(i) / n_clusters)\n",
        "        ax1.fill_betweenx(np.arange(y_lower, y_upper), 0, ith_cluster_silhouette_values, facecolor=color, edgecolor=color, alpha=0.7)\n",
        "\n",
        "        ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i)) # Label the silhouette plots with their cluster numbers at the middle\n",
        "        y_lower = y_upper + 10  # Compute the new y_lower for next plot. 10 for the 0 samples\n",
        "    plt.show()\n",
        "    \n",
        "        \n",
        "def silhouette(kmeans_dict, df, plot=False):\n",
        "    df = df.to_numpy()\n",
        "    avg_dict = dict()\n",
        "    for n_clusters, kmeans in kmeans_dict.items():      \n",
        "        kmeans_labels = kmeans.predict(df)\n",
        "        silhouette_avg = silhouette_score(df, kmeans_labels) # Average Score for all Samples\n",
        "        avg_dict.update( {silhouette_avg : n_clusters} )\n",
        "    \n",
        "        if(plot): plotSilhouette(df, n_clusters, kmeans_labels, silhouette_avg)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Running Kmeans\n",
        "k = 8\n",
        "kmeans_results = run_KMeans(k, final_df)\n",
        "\n",
        "# Plotting Silhouette Analysis\n",
        "#silhouette(kmeans_results, final_df, plot=True)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Cluster Analysis\n",
        "Now we can choose the best number of K and take a deeper look at each cluster. Looking at the plots above, we have some clues that when K = 5 is when the clusters are best defined. So first we will use a simple histogram to look at the most dominant words in each cluster:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_top_features_cluster(tf_idf_array, prediction, n_feats):\n",
        "    labels = np.unique(prediction)\n",
        "    dfs = []\n",
        "    for label in labels:\n",
        "        id_temp = np.where(prediction==label) # indices for each cluster\n",
        "        x_means = np.mean(tf_idf_array[id_temp], axis = 0) # returns average score across cluster\n",
        "        sorted_means = np.argsort(x_means)[::-1][:n_feats] # indices with top 20 scores\n",
        "        features = vectorizer.get_feature_names()\n",
        "        best_features = [(features[i], x_means[i]) for i in sorted_means]\n",
        "        df = pd.DataFrame(best_features, columns = ['features', 'score'])\n",
        "        dfs.append(df)\n",
        "    return dfs\n",
        "\n",
        "def plotWords(dfs, n_feats):\n",
        "    plt.figure(figsize=(8, 4))\n",
        "    for i in range(0, len(dfs)):\n",
        "        plt.title((\"Most Common Words in Cluster {}\".format(i)), fontsize=10, fontweight='bold')\n",
        "        sns.barplot(x = 'score' , y = 'features', orient = 'h' , data = dfs[i][:n_feats])\n",
        "        plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "best_result = 5\n",
        "kmeans = kmeans_results.get(best_result)\n",
        "\n",
        "final_df_array = final_df.to_numpy()\n",
        "prediction = kmeans.predict(final_df)\n",
        "n_feats = 20\n",
        "dfs = get_top_features_cluster(final_df_array, prediction, n_feats)\n",
        "plotWords(dfs, 13)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
